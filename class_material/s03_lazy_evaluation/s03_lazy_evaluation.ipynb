{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Lazy Evaluation and Query Optimization\n",
    "\n",
    "This session covers Polars' most powerful feature: **lazy evaluation**. You'll learn how to write optimized queries that can handle massive datasets efficiently.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. Understand the difference between eager and lazy execution\n",
    "2. Create and work with LazyFrames\n",
    "3. Inspect and understand query plans\n",
    "4. Apply query optimization techniques\n",
    "5. Use streaming for large datasets\n",
    "6. Benchmark Polars performance vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Eager vs Lazy Execution\n",
    "\n",
    "### What is Eager Execution?\n",
    "\n",
    "**Eager** execution means operations are performed immediately when called. This is how Pandas works, and it's the default mode for Polars DataFrames.\n",
    "\n",
    "```python\n",
    "# Each operation executes immediately\n",
    "df = pl.read_csv(\"data.csv\")  # Reads entire file now\n",
    "df = df.filter(...)            # Filters all rows now\n",
    "df = df.select(...)            # Selects columns now\n",
    "```\n",
    "\n",
    "### What is Lazy Execution?\n",
    "\n",
    "**Lazy** execution means operations are recorded but not executed until you explicitly request the result. This allows Polars to optimize the entire query plan.\n",
    "\n",
    "```python\n",
    "# Operations are recorded, not executed\n",
    "lf = pl.scan_csv(\"data.csv\")   # Doesn't read file yet\n",
    "lf = lf.filter(...)            # Adds filter to plan\n",
    "lf = lf.select(...)            # Adds select to plan\n",
    "df = lf.collect()              # NOW everything executes (optimized!)\n",
    "```\n",
    "\n",
    "### Benefits of Lazy Evaluation\n",
    "\n",
    "1. **Automatic optimization**: Polars can reorder and combine operations\n",
    "2. **Predicate pushdown**: Filters are pushed to the earliest possible point\n",
    "3. **Projection pushdown**: Only needed columns are read from disk\n",
    "4. **Memory efficiency**: Intermediate results aren't stored\n",
    "5. **Parallelization**: Operations can be automatically parallelized"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Pandas Comparison: Execution Models\n\n| Aspect | Pandas | Polars Eager | Polars Lazy |\n|--------|--------|--------------|-------------|\n| Execution | Immediate | Immediate | Deferred |\n| Optimization | None | None | Automatic |\n| Memory | Stores intermediates | Stores intermediates | Optimized |\n| File reading | Full file at once | Full file at once | Only needed columns |\n\n**Pandas equivalent of lazy evaluation**: Pandas doesn't have native lazy evaluation. The closest concept is using generators or Dask for deferred computation. Polars' lazy mode is a built-in feature that requires no additional libraries.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating LazyFrames\n",
    "\n",
    "There are two ways to create a LazyFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Scan a file (recommended for large files)\n",
    "lf = pl.scan_csv(\"data/large_transactions.csv\")\n",
    "print(f\"Type: {type(lf)}\")\n",
    "print(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Convert existing DataFrame to lazy\n",
    "df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n",
    "lf = df.lazy()\n",
    "print(f\"Type: {type(lf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Pandas Comparison: File Reading\n\n| Polars Lazy | Polars Eager | Pandas |\n|-------------|--------------|--------|\n| `pl.scan_csv()` | `pl.read_csv()` | `pd.read_csv()` |\n| `pl.scan_parquet()` | `pl.read_parquet()` | `pd.read_parquet()` |\n| `pl.scan_ndjson()` | `pl.read_json()` | `pd.read_json()` |\n\n**Key difference**: Pandas has no equivalent to `scan_*` functions. It always reads the entire file immediately. To achieve similar lazy behavior in Pandas, you would need external libraries like Dask (`dask.dataframe.read_csv()`).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences: scan_* vs read_*\n",
    "\n",
    "| Eager | Lazy | Description |\n",
    "|-------|------|-------------|\n",
    "| `pl.read_csv()` | `pl.scan_csv()` | CSV files |\n",
    "| `pl.read_parquet()` | `pl.scan_parquet()` | Parquet files |\n",
    "| `pl.read_json()` | `pl.scan_ndjson()` | JSON/NDJSON files |\n",
    "\n",
    "The `scan_*` functions don't read the entire file - they just set up a plan to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executing Queries: `collect()`\n",
    "\n",
    "The `collect()` method executes the lazy query and returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lazy query\n",
    "lf = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    .filter(pl.col(\"transaction_type\") == \"purchase\")\n",
    "    .filter(pl.col(\"amount\") > 100)\n",
    "    .select(\"transaction_id\", \"account_id\", \"amount\", \"merchant\")\n",
    ")\n",
    "\n",
    "# At this point, no computation has happened!\n",
    "print(\"Query built (not executed yet)\")\n",
    "print(f\"Type: {type(lf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "result = lf.collect()\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspecting Query Plans: `explain()`\n",
    "\n",
    "One of the most powerful features of lazy evaluation is the ability to see and understand the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a query\n",
    "lf = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    .filter(pl.col(\"transaction_type\") == \"purchase\")\n",
    "    .filter(pl.col(\"amount\") > 100)\n",
    "    .group_by(\"merchant\")\n",
    "    .agg(\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"amount\").sum().alias(\"total\")\n",
    "    )\n",
    "    .sort(\"total\", descending=True)\n",
    ")\n",
    "\n",
    "# Show the optimized plan\n",
    "print(\"OPTIMIZED QUERY PLAN:\")\n",
    "print(lf.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Pandas Comparison: Query Plans\n\n| Feature | Pandas | Polars |\n|---------|--------|--------|\n| View query plan | Not available | `lf.explain()` |\n| Optimize queries | Manual only | Automatic |\n| Predicate pushdown | Not available | Automatic |\n| Projection pushdown | Not available | Automatic |\n\n**Why Pandas can't do this**: Pandas executes operations immediately, so there's no \"plan\" to inspect or optimize. Each operation runs as soon as it's called. Polars' lazy evaluation records operations first, then optimizes the entire chain before execution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the unoptimized (naive) plan\n",
    "print(\"UNOPTIMIZED QUERY PLAN:\")\n",
    "print(lf.explain(optimized=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Query Plans\n",
    "\n",
    "Key elements in query plans:\n",
    "\n",
    "- **Csv SCAN**: Reading from CSV file\n",
    "- **SELECTION**: Filter predicates pushed down\n",
    "- **PROJECT**: Columns being read/selected\n",
    "- **AGGREGATE**: Groupby operations\n",
    "- **SORT**: Sorting operations\n",
    "\n",
    "Notice how the optimized plan pushes the filters down to the file scan level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Optimizations\n",
    "\n",
    "### 5.1 Predicate Pushdown\n",
    "\n",
    "Filters are pushed as close to the data source as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: These two filters will be combined and pushed to the scan\n",
    "lf = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    .with_columns(pl.col(\"amount\").abs().alias(\"abs_amount\"))\n",
    "    .filter(pl.col(\"transaction_type\") == \"purchase\")  # Filter 1\n",
    "    .select(\"transaction_id\", \"amount\", \"merchant\")\n",
    "    .filter(pl.col(\"amount\") > 200)  # Filter 2\n",
    ")\n",
    "\n",
    "print(lf.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Projection Pushdown\n",
    "\n",
    "Only the columns you actually need are read from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though the file has many columns, only 3 will be read\n",
    "lf = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    .select(\"transaction_id\", \"amount\", \"merchant\")\n",
    ")\n",
    "\n",
    "print(lf.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Common Subexpression Elimination\n",
    "\n",
    "Polars detects when the same computation is used multiple times and computes it only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean is computed once, not twice\n",
    "lf = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    .filter(pl.col(\"amount\") > 0)\n",
    "    .select(\n",
    "        pl.col(\"amount\"),\n",
    "        pl.col(\"amount\").mean().alias(\"avg_amount\"),\n",
    "        (pl.col(\"amount\") - pl.col(\"amount\").mean()).alias(\"diff_from_avg\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(lf.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking\n",
    "\n",
    "Let's compare performance between Pandas, Polars Eager, and Polars Lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pandas():\n",
    "    \"\"\"Benchmark Pandas operations.\"\"\"\n",
    "    df = pd.read_csv(\"data/large_transactions.csv\")\n",
    "    df = df[df[\"transaction_type\"] == \"purchase\"]\n",
    "    df = df[df[\"amount\"] > 100]\n",
    "    result = df.groupby(\"merchant\")[\"amount\"].agg([\"count\", \"sum\"])\n",
    "    result = result.sort_values(\"sum\", ascending=False)\n",
    "    return result\n",
    "\n",
    "def benchmark_polars_eager():\n",
    "    \"\"\"Benchmark Polars eager operations.\"\"\"\n",
    "    df = pl.read_csv(\"data/large_transactions.csv\")\n",
    "    df = df.filter(pl.col(\"transaction_type\") == \"purchase\")\n",
    "    df = df.filter(pl.col(\"amount\") > 100)\n",
    "    result = df.group_by(\"merchant\").agg(\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"amount\").sum().alias(\"sum\")\n",
    "    ).sort(\"sum\", descending=True)\n",
    "    return result\n",
    "\n",
    "def benchmark_polars_lazy():\n",
    "    \"\"\"Benchmark Polars lazy operations.\"\"\"\n",
    "    result = (\n",
    "        pl.scan_csv(\"data/large_transactions.csv\")\n",
    "        .filter(pl.col(\"transaction_type\") == \"purchase\")\n",
    "        .filter(pl.col(\"amount\") > 100)\n",
    "        .group_by(\"merchant\")\n",
    "        .agg(\n",
    "            pl.len().alias(\"count\"),\n",
    "            pl.col(\"amount\").sum().alias(\"sum\")\n",
    "        )\n",
    "        .sort(\"sum\", descending=True)\n",
    "        .collect()\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "print(\"Running benchmarks (3 iterations each)...\\n\")\n",
    "\n",
    "# Pandas benchmark\n",
    "times_pandas = []\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    result_pandas = benchmark_pandas()\n",
    "    times_pandas.append(time.time() - start)\n",
    "\n",
    "# Polars eager benchmark\n",
    "times_eager = []\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    result_eager = benchmark_polars_eager()\n",
    "    times_eager.append(time.time() - start)\n",
    "\n",
    "# Polars lazy benchmark\n",
    "times_lazy = []\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    result_lazy = benchmark_polars_lazy()\n",
    "    times_lazy.append(time.time() - start)\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Method':<20} {'Avg Time (s)':<15} {'Speedup vs Pandas'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "avg_pandas = sum(times_pandas) / len(times_pandas)\n",
    "avg_eager = sum(times_eager) / len(times_eager)\n",
    "avg_lazy = sum(times_lazy) / len(times_lazy)\n",
    "\n",
    "print(f\"{'Pandas':<20} {avg_pandas:<15.4f} {'1.0x (baseline)'}\")\n",
    "print(f\"{'Polars (eager)':<20} {avg_eager:<15.4f} {avg_pandas/avg_eager:.1f}x\")\n",
    "print(f\"{'Polars (lazy)':<20} {avg_lazy:<15.4f} {avg_pandas/avg_lazy:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Pandas Comparison: Large Dataset Processing\n\n| Approach | Pandas | Polars |\n|----------|--------|--------|\n| Process in chunks | `pd.read_csv(chunksize=1000)` | `lf.collect(streaming=True)` |\n| Memory efficiency | Manual chunk management | Automatic |\n| Code complexity | Requires iteration logic | Single line change |\n\n```python\n# Pandas chunked processing (manual)\nchunks = []\nfor chunk in pd.read_csv(\"large.csv\", chunksize=10000):\n    result = chunk.groupby(\"col\").sum()\n    chunks.append(result)\nfinal = pd.concat(chunks).groupby(level=0).sum()\n\n# Polars streaming (automatic)\nresult = (\n    pl.scan_csv(\"large.csv\")\n    .group_by(\"col\").agg(pl.sum(\"value\"))\n    .collect(streaming=True)\n)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming for Large Datasets\n",
    "\n",
    "For datasets that don't fit in memory, Polars supports **streaming** execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming processes data in chunks\n",
    "result = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    .filter(pl.col(\"amount\") > 0)\n",
    "    .group_by(\"transaction_type\")\n",
    "    .agg(\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"amount\").sum().alias(\"total\"),\n",
    "        pl.col(\"amount\").mean().alias(\"avg\")\n",
    "    )\n",
    "    .collect(streaming=True)  # Enable streaming\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Streaming\n",
    "\n",
    "Use `collect(streaming=True)` when:\n",
    "- Dataset is larger than available memory\n",
    "- You want to limit memory usage\n",
    "- Processing very large files\n",
    "\n",
    "Note: Not all operations support streaming. Polars will fall back to non-streaming if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Window Functions with `over()`\n",
    "\n",
    "Window functions allow calculations across groups without reducing the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "transactions = pl.read_csv(\"data/large_transactions.csv\").head(1000)  # Sample for demo\n",
    "\n",
    "# Window functions: compute stats per group while keeping all rows\n",
    "result = transactions.with_columns(\n",
    "    # Average amount per account\n",
    "    pl.col(\"amount\").mean().over(\"account_id\").alias(\"account_avg\"),\n",
    "    \n",
    "    # Rank within each account (by amount)\n",
    "    pl.col(\"amount\").rank().over(\"account_id\").alias(\"rank_in_account\"),\n",
    "    \n",
    "    # Count of transactions per account\n",
    "    pl.len().over(\"account_id\").alias(\"account_tx_count\"),\n",
    "    \n",
    "    # Difference from account average\n",
    "    (pl.col(\"amount\") - pl.col(\"amount\").mean().over(\"account_id\")).alias(\"diff_from_avg\")\n",
    ")\n",
    "\n",
    "result.select(\n",
    "    \"transaction_id\", \"account_id\", \"amount\", \n",
    "    \"account_avg\", \"rank_in_account\", \"account_tx_count\", \"diff_from_avg\"\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Comparison: Window Functions\n",
    "\n",
    "```python\n",
    "# Pandas uses transform with groupby\n",
    "df[\"account_avg\"] = df.groupby(\"account_id\")[\"amount\"].transform(\"mean\")\n",
    "df[\"rank\"] = df.groupby(\"account_id\")[\"amount\"].rank()\n",
    "```\n",
    "\n",
    "Polars' `.over()` is more flexible and integrates naturally with the expression API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices for Query Optimization\n",
    "\n",
    "### DO:\n",
    "1. **Use lazy mode** (`scan_*`) for large files\n",
    "2. **Filter early** - put filters before expensive operations\n",
    "3. **Select only needed columns** - projection pushdown saves I/O\n",
    "4. **Chain operations** - allows better optimization\n",
    "5. **Use `explain()`** to understand your query plan\n",
    "6. **Use Parquet** instead of CSV for large datasets\n",
    "\n",
    "### DON'T:\n",
    "1. **Don't call `collect()` multiple times** - execute once at the end\n",
    "2. **Don't mix eager and lazy unnecessarily**\n",
    "3. **Don't use `apply()` with Python functions** - it's slow\n",
    "4. **Don't iterate row by row** - use vectorized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Good query structure\n",
    "good_query = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    # Filter early (pushdown)\n",
    "    .filter(pl.col(\"transaction_type\") == \"purchase\")\n",
    "    .filter(pl.col(\"amount\") > 50)\n",
    "    # Select only needed columns (projection pushdown)\n",
    "    .select(\"account_id\", \"amount\", \"merchant\")\n",
    "    # Aggregate\n",
    "    .group_by(\"merchant\")\n",
    "    .agg(\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"amount\").sum().alias(\"total\")\n",
    "    )\n",
    "    .sort(\"total\", descending=True)\n",
    "    .head(10)\n",
    "    # Execute once at the end\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "good_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practical Example: Full Analysis Pipeline\n",
    "\n",
    "Let's build a complete analysis pipeline using lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive transaction analysis\n",
    "analysis = (\n",
    "    pl.scan_csv(\"data/large_transactions.csv\")\n",
    "    # Parse timestamp\n",
    "    .with_columns(\n",
    "        pl.col(\"timestamp\").str.to_datetime().alias(\"datetime\")\n",
    "    )\n",
    "    # Extract date components\n",
    "    .with_columns(\n",
    "        pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "        pl.col(\"datetime\").dt.hour().alias(\"hour\")\n",
    "    )\n",
    "    # Filter to purchases only\n",
    "    .filter(pl.col(\"transaction_type\") == \"purchase\")\n",
    "    # Aggregate by month\n",
    "    .group_by(\"month\")\n",
    "    .agg(\n",
    "        pl.len().alias(\"transaction_count\"),\n",
    "        pl.col(\"amount\").sum().alias(\"total_revenue\"),\n",
    "        pl.col(\"amount\").mean().alias(\"avg_transaction\"),\n",
    "        pl.col(\"account_id\").n_unique().alias(\"unique_customers\"),\n",
    "        pl.col(\"is_flagged\").sum().alias(\"flagged_count\")\n",
    "    )\n",
    "    .sort(\"month\")\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"Monthly Analysis:\")\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Lazy Evaluation Cheat Sheet\n",
    "\n",
    "| Eager | Lazy | Description |\n",
    "|-------|------|-------------|\n",
    "| `pl.read_csv()` | `pl.scan_csv()` | Read/scan CSV |\n",
    "| `pl.read_parquet()` | `pl.scan_parquet()` | Read/scan Parquet |\n",
    "| `DataFrame` | `LazyFrame` | Data structure |\n",
    "| Operations execute immediately | Operations are deferred | Execution model |\n",
    "| N/A | `lf.collect()` | Execute lazy query |\n",
    "| N/A | `lf.explain()` | View query plan |\n",
    "| `df.lazy()` | N/A | Convert to lazy |\n",
    "\n",
    "### Key Optimizations\n",
    "\n",
    "- **Predicate pushdown**: Filters applied at scan time\n",
    "- **Projection pushdown**: Only needed columns read\n",
    "- **CSE**: Common subexpressions computed once\n",
    "- **Streaming**: Process larger-than-memory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Write a lazy query that finds the top 5 merchants by transaction count\n",
    "2. Use `explain()` to compare the optimized vs unoptimized plan\n",
    "3. Benchmark lazy vs eager for a groupby aggregation on the full dataset\n",
    "4. Use window functions to calculate each transaction's percentile within its account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Top 5 merchants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compare plans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Window percentiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Conclusion\n",
    "\n",
    "Congratulations! You've completed the Advanced Tech Track on Polars. You've learned:\n",
    "\n",
    "1. **Session 1**: Polars basics, DataFrames, expressions, and the Pandas comparison\n",
    "2. **Session 2**: Data manipulation - filtering, transformations, groupby, joins\n",
    "3. **Session 3**: Lazy evaluation, query optimization, and performance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Practice with your own datasets\n",
    "- Explore the [Polars documentation](https://docs.pola.rs/)\n",
    "- Join the [Polars Discord community](https://discord.gg/4UfP5cfBE7)\n",
    "- Try more advanced features: custom expressions, plugins, cloud integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}